{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a77c8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page https://seekingalpha.com/earnings/earnings-call-transcripts/1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def get_date(c):\n",
    "    end = c.find('|')\n",
    "    date_str = c[0:end-1].strip()\n",
    "    return datetime.strptime(date_str, '%B %d, %Y')  # Convert to datetime object\n",
    "\n",
    "def get_ticker(c):\n",
    "    beg = c.find('(')\n",
    "    end = c.find(')')\n",
    "    return c[beg+1:end]\n",
    "\n",
    "def grab_page(url, start_date, end_date):\n",
    "    print(\"Attempting to grab page: \" + url)\n",
    "    page = requests.get(url)\n",
    "    page_html = page.text\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "    # Find metadata for the transcript date and ticker symbol\n",
    "    meta = soup.find(\"div\", {'class': 'a-info get-alerts'})\n",
    "    content = soup.find(id=\"a-body\")\n",
    "\n",
    "    # Ensure both metadata and content exist before proceeding\n",
    "    if not meta or not content:\n",
    "        print(\"Skipping this link, no content here\")\n",
    "        return\n",
    "\n",
    "    # Extract transcript date\n",
    "    mtext = meta.text\n",
    "    transcript_date = get_date(mtext)\n",
    "\n",
    "    # Only save the transcript if it's within the specified date range\n",
    "    if start_date <= transcript_date <= end_date:\n",
    "        # Get ticker and formatted date for filename\n",
    "        filename = get_ticker(mtext) + \"_\" + transcript_date.strftime('%Y-%m-%d')\n",
    "        with open(filename.lower() + \".txt\", 'w', encoding='utf-8') as file:\n",
    "            file.write(content.get_text())  # Write text content of the transcript\n",
    "        print(filename.lower() + \" successfully saved\")\n",
    "    else:\n",
    "        print(\"Transcript date out of range, skipping.\")\n",
    "\n",
    "def process_list_page(i, start_date, end_date):\n",
    "    origin_page = f\"https://seekingalpha.com/earnings/earnings-call-transcripts/{i}\"\n",
    "    print(\"Getting page \" + origin_page)\n",
    "    page = requests.get(origin_page)\n",
    "    page_html = page.text\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    \n",
    "    # Find all list items for articles and extract their links\n",
    "    articles = soup.find_all(\"li\", {'class': 'list-group-item article'})\n",
    "    for article in articles:\n",
    "        article_link = article.find(\"a\", href=True)\n",
    "        if article_link:\n",
    "            url = \"https://seekingalpha.com\" + article_link['href']\n",
    "            grab_page(url, start_date, end_date)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "# Set your date range\n",
    "start_date = datetime(2024, 6, 30)\n",
    "end_date = datetime(2024, 7, 31)\n",
    "\n",
    "# Choose what pages of earnings to scrape\n",
    "for i in range(1, 2): \n",
    "    process_list_page(i, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a697fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page https://seekingalpha.com/earnings/earnings-call-transcripts/1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def get_date(c):\n",
    "    end = c.find('|')\n",
    "    date_str = c[0:end-1].strip()\n",
    "    return datetime.strptime(date_str, '%B %d, %Y')  # Convert to datetime object\n",
    "\n",
    "def get_ticker(c):\n",
    "    beg = c.find('(')\n",
    "    end = c.find(')')\n",
    "    return c[beg+1:end]\n",
    "\n",
    "def grab_page(url, start_date, end_date):\n",
    "    print(\"Attempting to grab page: \" + url)\n",
    "    page = requests.get(url)\n",
    "    page_html = page.text\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "    # Find metadata for the transcript date and ticker symbol\n",
    "    meta = soup.find(\"div\", {'class': 'a-info get-alerts'})\n",
    "\n",
    "    # Attempt to locate transcript text in different possible containers\n",
    "    content = soup.find(\"div\", {'class': 'sa-art'})\n",
    "\n",
    "    if not meta or not content:\n",
    "        print(\"Skipping this link, no content here\")\n",
    "        return\n",
    "\n",
    "    # Extract transcript date\n",
    "    mtext = meta.text\n",
    "    transcript_date = get_date(mtext)\n",
    "\n",
    "    # Only save the transcript if it's within the specified date range\n",
    "    if start_date <= transcript_date <= end_date:\n",
    "        # Get ticker and formatted date for filename\n",
    "        filename = get_ticker(mtext) + \"_\" + transcript_date.strftime('%Y-%m-%d')\n",
    "        with open(filename.lower() + \".txt\", 'w', encoding='utf-8') as file:\n",
    "            file.write(content.get_text())  # Write text content of the transcript\n",
    "        print(filename.lower() + \" successfully saved\")\n",
    "    else:\n",
    "        print(\"Transcript date out of range, skipping.\")\n",
    "\n",
    "def process_list_page(i, start_date, end_date):\n",
    "    origin_page = f\"https://seekingalpha.com/earnings/earnings-call-transcripts/{i}\"\n",
    "    print(\"Getting page \" + origin_page)\n",
    "    page = requests.get(origin_page)\n",
    "    page_html = page.text\n",
    "    soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    \n",
    "    # Find all list items for articles and extract their links\n",
    "    articles = soup.find_all(\"li\", {'class': 'list-group-item article'})\n",
    "    for article in articles:\n",
    "        article_link = article.find(\"a\", href=True)\n",
    "        if article_link:\n",
    "            url = \"https://seekingalpha.com\" + article_link['href']\n",
    "            grab_page(url, start_date, end_date)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "# Set your date range\n",
    "start_date = datetime(2024, 6, 30)\n",
    "end_date = datetime(2024, 7, 31)\n",
    "\n",
    "# Choose what pages of earnings to scrape\n",
    "for i in range(1, 2): \n",
    "    process_list_page(i, start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ac10599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'You are not subscribed to this API.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://seeking-alpha.p.rapidapi.com/transcripts/v2/list\"\n",
    "\n",
    "querystring = {\"id\":\"aapl\",\"size\":\"20\",\"number\":\"1\"}\n",
    "\n",
    "headers = {\n",
    "    \"x-rapidapi-key\": \"Sign Up for Key\",\n",
    "    \"x-rapidapi-host\": \"seeking-alpha.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105b026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
